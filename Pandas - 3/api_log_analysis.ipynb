{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7621abc",
   "metadata": {},
   "source": [
    "# API Server Log Analysis with Pandas\n",
    "\n",
    "This notebook performs comprehensive analysis on API server logs, covering data loading, cleaning, filtering, aggregation, datetime processing, and business insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed522462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f100ef",
   "metadata": {},
   "source": [
    "## Part 1: Pandas Essentials - Q1. Load & Inspect Data\n",
    "\n",
    "Load the CSV file into a Pandas DataFrame and display first 5 rows, column names, data types, and total rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a56244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('api_logs.csv')\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Total number of rows\n",
    "print(f\"\\nTotal number of rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4b7be",
   "metadata": {},
   "source": [
    "## Part 1: Pandas Essentials - Q2. Series & DataFrame Operations\n",
    "\n",
    "Extract the response_time_ms column as a Series, calculate mean, median, and 95th percentile, and add a new column for response_time_sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9449ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract response_time_ms as Series\n",
    "response_times = df['response_time_ms']\n",
    "\n",
    "# Calculate statistics\n",
    "mean_rt = response_times.mean()\n",
    "median_rt = response_times.median()\n",
    "p95_rt = response_times.quantile(0.95)\n",
    "\n",
    "print(f\"Mean response time: {mean_rt:.2f} ms\")\n",
    "print(f\"Median response time: {median_rt:.2f} ms\")\n",
    "print(f\"95th percentile response time: {p95_rt:.2f} ms\")\n",
    "\n",
    "# Add new column response_time_sec\n",
    "df['response_time_sec'] = df['response_time_ms'] / 1000\n",
    "\n",
    "print(\"\\nDataFrame with new column:\")\n",
    "print(df[['response_time_ms', 'response_time_sec']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330cf73",
   "metadata": {},
   "source": [
    "## Part 1: Pandas Essentials - Q3. Reading JSON\n",
    "\n",
    "Load the JSON file into a DataFrame and merge it with the logs DataFrame on the endpoint column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "with open('endpoint_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "metadata_df = pd.DataFrame(list(metadata.items()), columns=['endpoint', 'service_name'])\n",
    "\n",
    "print(\"Endpoint metadata:\")\n",
    "print(metadata_df)\n",
    "\n",
    "# Merge with logs DataFrame\n",
    "df = df.merge(metadata_df, on='endpoint', how='left')\n",
    "\n",
    "print(\"\\nMerged DataFrame (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82439116",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning & Filtering - Q4. Handling Missing Values\n",
    "\n",
    "Identify missing values, fill user_agent with 'unknown', drop rows with missing response_time_ms, and explain fillna vs dropna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing user_agent with 'unknown'\n",
    "df['user_agent'] = df['user_agent'].fillna('unknown')\n",
    "\n",
    "# Drop rows where response_time_ms is missing\n",
    "df = df.dropna(subset=['response_time_ms'])\n",
    "\n",
    "print(f\"\\nAfter cleaning, total rows: {len(df)}\")\n",
    "\n",
    "# Explanation\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"Use fillna when the missing values can be reasonably imputed (e.g., categorical data like user_agent).\")\n",
    "print(\"Use dropna when missing values are critical and cannot be imputed (e.g., response_time_ms for analysis).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78db39d",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning & Filtering - Q5. Filtering Rows\n",
    "\n",
    "Filter rows for status_code >= 400, specific endpoints (/login, /checkout), and response_time_ms > 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter requests with status_code >= 400\n",
    "error_requests = df[df['status_code'] >= 400]\n",
    "print(f\"Error requests (status >= 400): {len(error_requests)}\")\n",
    "\n",
    "# Filter logs for /login and /checkout\n",
    "login_checkout = df[df['endpoint'].isin(['/login', '/checkout'])]\n",
    "print(f\"Login/Checkout requests: {len(login_checkout)}\")\n",
    "\n",
    "# Filter requests with response_time_ms > 500\n",
    "slow_requests = df[df['response_time_ms'] > 500]\n",
    "print(f\"Slow requests (>500ms): {len(slow_requests)}\")\n",
    "\n",
    "print(\"\\nSample slow requests:\")\n",
    "print(slow_requests[['endpoint', 'response_time_ms', 'status_code']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061d528",
   "metadata": {},
   "source": [
    "## Part 3: Sorting, Grouping & Aggregation - Q6. Sorting\n",
    "\n",
    "Sort logs by response_time_ms descending and display top 10 slowest calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471183fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by response_time_ms descending\n",
    "sorted_df = df.sort_values('response_time_ms', ascending=False)\n",
    "\n",
    "# Display top 10 slowest API calls\n",
    "print(\"Top 10 slowest API calls:\")\n",
    "print(sorted_df[['endpoint', 'response_time_ms', 'status_code', 'ip_address']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d15c4",
   "metadata": {},
   "source": [
    "## Part 3: Sorting, Grouping & Aggregation - Q7. Grouping & Aggregation\n",
    "\n",
    "Group by endpoint for average response time and request count, by status_code for counts, and find endpoint with highest average latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by endpoint\n",
    "endpoint_stats = df.groupby('endpoint').agg(\n",
    "    avg_response_time=('response_time_ms', 'mean'),\n",
    "    total_requests=('endpoint', 'count')\n",
    ")\n",
    "print(\"Endpoint statistics:\")\n",
    "print(endpoint_stats)\n",
    "\n",
    "# Group by status_code\n",
    "status_counts = df.groupby('status_code').size()\n",
    "print(\"\\nStatus code counts:\")\n",
    "print(status_counts)\n",
    "\n",
    "# Find endpoint with highest average latency\n",
    "highest_latency_endpoint = endpoint_stats['avg_response_time'].idxmax()\n",
    "highest_latency = endpoint_stats['avg_response_time'].max()\n",
    "print(f\"\\nEndpoint with highest average latency: {highest_latency_endpoint} ({highest_latency:.2f} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299f2e2",
   "metadata": {},
   "source": [
    "## Part 4: Working with Datetime - Q8. Datetime Processing\n",
    "\n",
    "Convert timestamp to datetime, extract date and hour, and create a request_hour column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce32cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Extract date and hour\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Create request_hour column\n",
    "df['request_hour'] = df['timestamp'].dt.floor('H')\n",
    "\n",
    "print(\"DataFrame with datetime columns:\")\n",
    "print(df[['timestamp', 'date', 'hour', 'request_hour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6183de",
   "metadata": {},
   "source": [
    "## Part 4: Working with Datetime - Q9. Time-Based Analysis\n",
    "\n",
    "Calculate requests per hour, find peak hour, and plot volume per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate requests per hour\n",
    "requests_per_hour = df.groupby('request_hour').size()\n",
    "print(\"Requests per hour:\")\n",
    "print(requests_per_hour)\n",
    "\n",
    "# Find peak traffic hour\n",
    "peak_hour = requests_per_hour.idxmax()\n",
    "peak_count = requests_per_hour.max()\n",
    "print(f\"\\nPeak traffic hour: {peak_hour} with {peak_count} requests\")\n",
    "\n",
    "# Plot request volume per hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "requests_per_hour.plot(kind='bar')\n",
    "plt.title('Request Volume per Hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Requests')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec3a84",
   "metadata": {},
   "source": [
    "## Part 5: Real-World Log Processing - Q10. Log Metrics â€“ Latency\n",
    "\n",
    "Calculate overall average latency, 90th and 99th percentiles, and average latency per endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7514b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall average latency\n",
    "overall_avg = df['response_time_ms'].mean()\n",
    "print(f\"Overall average latency: {overall_avg:.2f} ms\")\n",
    "\n",
    "# 90th and 99th percentiles\n",
    "p90 = df['response_time_ms'].quantile(0.90)\n",
    "p99 = df['response_time_ms'].quantile(0.99)\n",
    "print(f\"90th percentile latency: {p90:.2f} ms\")\n",
    "print(f\"99th percentile latency: {p99:.2f} ms\")\n",
    "\n",
    "# Average latency per endpoint\n",
    "avg_latency_per_endpoint = df.groupby('endpoint')['response_time_ms'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage latency per endpoint:\")\n",
    "print(avg_latency_per_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d0297",
   "metadata": {},
   "source": [
    "## Part 5: Real-World Log Processing - Q11. Throughput Calculation\n",
    "\n",
    "Calculate requests per minute, average throughput, and peak throughput minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minute column\n",
    "df['request_minute'] = df['timestamp'].dt.floor('T')\n",
    "\n",
    "# Calculate requests per minute\n",
    "throughput_per_minute = df.groupby('request_minute').size()\n",
    "\n",
    "# Average throughput\n",
    "avg_throughput = throughput_per_minute.mean()\n",
    "print(f\"Average throughput: {avg_throughput:.2f} requests per minute\")\n",
    "\n",
    "# Peak throughput minute\n",
    "peak_minute = throughput_per_minute.idxmax()\n",
    "peak_throughput = throughput_per_minute.max()\n",
    "print(f\"Peak throughput minute: {peak_minute} with {peak_throughput} requests\")\n",
    "\n",
    "print(\"\\nThroughput per minute (sample):\")\n",
    "print(throughput_per_minute.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd409775",
   "metadata": {},
   "source": [
    "## Part 6: Business Insight Questions - Q12. IP Analysis\n",
    "\n",
    "Identify top 10 IPs by request count and detect suspicious IPs with >1000 requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e578f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 IP addresses by request count\n",
    "top_ips = df.groupby('ip_address').size().sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 IP addresses by request count:\")\n",
    "print(top_ips)\n",
    "\n",
    "# Detect suspicious IPs (>1000 requests)\n",
    "suspicious_ips = df.groupby('ip_address').size()\n",
    "suspicious_ips = suspicious_ips[suspicious_ips > 1000]\n",
    "print(f\"\\nSuspicious IPs (>1000 requests): {len(suspicious_ips)}\")\n",
    "if len(suspicious_ips) > 0:\n",
    "    print(suspicious_ips)\n",
    "else:\n",
    "    print(\"No IPs with >1000 requests found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c9b2c",
   "metadata": {},
   "source": [
    "## Part 6: Business Insight Questions - Q13. Insights & Interpretation\n",
    "\n",
    "Provide plain English answers on performance bottlenecks, traffic spikes, error correlations, and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insights & Interpretation\n",
    "\n",
    "print(\"### Which endpoints are performance bottlenecks?\")\n",
    "print(f\"Based on the analysis, the endpoint with the highest average latency is {highest_latency_endpoint} with {highest_latency:.2f} ms.\")\n",
    "print(\"Endpoints with high latency should be investigated for optimization.\")\n",
    "\n",
    "print(\"\\n### At what time does traffic spike the most?\")\n",
    "print(f\"Traffic spikes at {peak_hour} with {peak_count} requests.\")\n",
    "\n",
    "print(\"\\n### Are errors correlated with high response times?\")\n",
    "error_correlation = df[df['status_code'] >= 400]['response_time_ms'].mean()\n",
    "success_correlation = df[df['status_code'] < 400]['response_time_ms'].mean()\n",
    "print(f\"Average response time for errors: {error_correlation:.2f} ms\")\n",
    "print(f\"Average response time for successful requests: {success_correlation:.2f} ms\")\n",
    "if error_correlation > success_correlation:\n",
    "    print(\"Errors tend to have higher response times, indicating correlation.\")\n",
    "else:\n",
    "    print(\"No clear correlation between errors and response times.\")\n",
    "\n",
    "print(\"\\n### What recommendations would you give to backend teams?\")\n",
    "print(\"1. Optimize the high-latency endpoint:\", highest_latency_endpoint)\n",
    "print(\"2. Monitor traffic during peak hours:\", peak_hour)\n",
    "print(\"3. Investigate error-prone requests to reduce failures.\")\n",
    "print(\"4. Implement rate limiting for suspicious IPs.\")\n",
    "print(\"5. Consider caching for frequently accessed endpoints.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
